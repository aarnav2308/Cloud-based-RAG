üîç Project Overview

This project demonstrates an end-to-end Retrieval-Augmented Generation (RAG) system built using cloud infrastructure, vector databases, and local large language models.

The system retrieves relevant information from domain-specific documents using semantic search and generates accurate, grounded responses using a locally hosted LLM. The architecture mirrors real-world production RAG systems used in enterprise AI applications.

‚úÖ Skills Demonstrated:

Cloud-native AI architecture

Vector databases & semantic search

Retrieval-Augmented Generation (RAG)

Local LLM inference (Ollama)

Python backend development

Debugging, observability & system validation

Git/GitHub project hygiene

üß† What This System Does

Converts documents into embeddings

Stores embeddings in a vector database (Pinecone)

Retrieves the most relevant chunks using similarity search

Constructs a grounded prompt from retrieved context

Generates answers using a local LLM

Returns answers with sources and similarity scores
